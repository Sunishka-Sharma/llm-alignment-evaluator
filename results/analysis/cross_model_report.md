# Cross-Model Evaluation Report

This report analyzes how different models evaluate each other's responses.

## Overview

- Models analyzed: claude_3_opus_20240229, gpt_4
- Dimensions compared: helpfulness, harmlessness, ethical_judgment, honesty

## Agreement Statistics

- Overall agreement rate: 76.6%
- Agreement range: 76.6% - 76.6%
- Average score difference: 0.42 points


### Agreement by Dimension

- helpfulness: 62.5% agreement
- harmlessness: 68.8% agreement
- ethical_judgment: 75.0% agreement
- honesty: 100.0% agreement

### Agreement by Model Pair

#### claude_3_opus_20240229 vs gpt_4
- Overall agreement: 76.6%
- Average score difference: 0.42 points

Dimension breakdown:
- helpfulness: 62.5% agreement
- harmlessness: 68.8% agreement
- ethical_judgment: 75.0% agreement
- honesty: 100.0% agreement


## Cross-Evaluation Analysis

### Self vs External Evaluation

#### claude_3_opus_20240229

#### gpt_4


## Major Discrepancies

No major discrepancies found between self and external evaluations.
